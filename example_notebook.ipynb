{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ChemSpaceAL\n",
    "from ChemSpaceAL import InitializeWorkspace\n",
    "from ChemSpaceAL import Configuration\n",
    "from ChemSpaceAL import Dataset\n",
    "from ChemSpaceAL import Model\n",
    "from ChemSpaceAL import Training\n",
    "from ChemSpaceAL import Generation\n",
    "from ChemSpaceAL import Sampling\n",
    "from ChemSpaceAL import ALConstruction\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    will create folders at base_path='/Users/morgunov/batista/Summer/ChemSpaceAL/publication_runs/'\n",
      "--- The following previously scored molecules were set:\n",
      "     4_Scoring/scored_dataframes/model0_al0_ch1.csv\n",
      "--- The following previously constructed Active Learning sets were set:\n",
      "     5_ActiveLearning/training_sets/model0_al0_ch1.csv\n"
     ]
    }
   ],
   "source": [
    "# Intiialize the workspace directory\n",
    "base_path = os.getcwd() + \"/publication_runs/\"\n",
    "InitializeWorkspace.create_folders(base_path=base_path)\n",
    "config = Configuration.Config(\n",
    "    base_path=base_path,\n",
    "    cycle_prefix=\"model0\",\n",
    "    al_iteration=1,\n",
    "    cycle_suffix=\"ch1\",\n",
    "    training_fname=\"moses_train.csv.gz\",\n",
    "    validation_fname=\"moses_test.csv.gz\",\n",
    "    slice_data=1_000,\n",
    "    verbose=True,\n",
    ")\n",
    "config.set_previous_arrays()\n",
    "# ds = Dataset.load_data(config=config, mode=\"Pretraining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The following training parameters were set:\n",
      "    number of epochs: 1\n",
      "    learning rate: 0.0003\n",
      "    learning warmup enabled? True\n",
      "    model weights will be saved to:               1_Pretraining/model_weights/model0_al1_ch1.pt                                   \n",
      "    dataset descriptors will be loaded from:      1_Pretraining/datasets_descriptors/moses_train.yaml                             \n",
      "  . note: wandb_project_name and wandb_runname were not provided, you can ignore this message if you don't plan to log runs to wandb\n",
      "--- The following generation parameters were set:\n",
      "    target number: 1000 unique canonical smiles that pass filters\n",
      "    batch size: 64 & temperature: 1.0\n",
      "    the following filters will be applied: ADMET+FGs\n",
      "    model weights will be loaded from:            1_Pretraining/model_weights/model0_al1_ch1.pt                                   \n",
      "    dataset descriptors will be loaded from:      1_Pretraining/datasets_descriptors/moses_train.yaml                             \n",
      "    generated completions will be saved to:       2_Generation/model0_al1_ch1_completions.csv                                     \n",
      "    unique canonic smiles will be saved to:       2_Generation/model0_al1_ch1_unique_smiles.csv                                   \n",
      "    generation metrics will be saved to:          2_Generation/model0_al1_ch1_metrics.txt                                         \n",
      "    filtered molecules will be saved to:          2_Generation/model0_al1_ch1_filtered_smiles.csv                                 \n",
      "    The following ADMET filters will be enforced:\n",
      "    |    MW in range [100, 600]\n",
      "    |    nHA in range [0, 12]\n",
      "    |    nHD in range [0, 7]\n",
      "    |    nRot in range [0, 11]\n",
      "    |    nRing in range [0, 6]\n",
      "    |    nHet in range [1, 15]\n",
      "    |    fChar in range [-4, 4]\n",
      "    |    TPSA in range [0, 140]\n",
      "    |    logP in range [-0.4, 6.5]\n",
      "    The following functional groups will be restricted:\n",
      "    |    fr_azide, fr_isocyan, fr_isothiocyan, fr_nitro, fr_nitro_arom,\n",
      "    |    fr_nitro_arom_nonortho, fr_nitroso, fr_phos_acid, fr_phos_ester,\n",
      "    |    fr_sulfonamd, fr_sulfone, fr_term_acetylene, fr_thiocyan,\n",
      "    |    fr_prisulfonamd, fr_C_S, fr_azo, fr_diazo, fr_epoxide, fr_ester,\n",
      "    |    fr_COO2, fr_Imine, fr_N_O, fr_SH, fr_aldehyde, fr_dihydropyridine,\n",
      "    |    fr_hdrzine, fr_hdrzone, fr_ketone, fr_thiophene, fr_phenol\n",
      "--- The following sampling parameters were set:\n",
      "    number of clusters: 10\n",
      "    samples per cluster: 2\n",
      "    descriptors mode: mix\n",
      "    descriptors will be saved to:                      3_Sampling/generations_descriptors/model0_al1_ch1.pkl                           \n",
      "    PCA will be loaded from:                           3_Sampling/pca_weights/scaler_pca_combined_processed_freq1000_block133_120.pkl  \n",
      "    KMeans Objects will be saved to:                   3_Sampling/kmeans_objects/model0_al1_ch1_k10.pkl                                \n",
      "    cluster to molecules mapping will be saved to:     3_Sampling/clusterings/model0_al1_ch1_cluster_to_mols.pkl                       \n",
      "    sampled molecules will be saved to:                4_Scoring/sampled_mols/model0_al1_ch1_sampled20.csv                             \n",
      "--- The following scoring parameters were set:\n",
      "    Reminder that docking poses will be written to 4_Scoring/binding_poses/                                                        \n",
      "    protein will be loaded from                   4_Scoring/binding_targets/1ieb_processed.pdb                                    \n",
      "    poses will be saved to                        4_Scoring/binding_poses/model0_al1_ch1/                                         \n",
      "    and scored molecules will be saved to         4_Scoring/scored_dataframes/model0_al1_ch1.csv                                  \n",
      "    The following prolif interaction weights will be used:\n",
      "    |    Hydrophobic: 2.5, HBDonor: 3.5, HBAcceptor: 3.5, Anionic: 7.5, Cationic: 7.5,\n",
      "    |    CationPi: 2.5, PiCation: 2.5, VdWContact: 1.0, XBAcceptor: 3.0,\n",
      "    |    XBDonor: 3.0, FaceToFace: 3.0, EdgeToFace: 1.0, MetalDonor: 3.0,\n",
      "    |    MetalAcceptor: 3.0\n",
      "--- The following AL training set construction parameters were set:\n",
      "    the training set will be constructed to have 10000 molecules\n",
      "    of which 5000 will be selected from the top scoring molecules defined by the following parameters:\n",
      "        molecules with score above 11 will be selected\n",
      "    the remaining 5000 molecules will be selected from high-scoring clusters according to the following parameters:\n",
      "        the following probability mode will be used: softmax\n",
      "    the training set will be saved to             5_ActiveLearning/training_sets/model0_al1_ch1.csv                               \n"
     ]
    }
   ],
   "source": [
    "config.verbose = True\n",
    "config.set_training_parameters(mode=\"Pretraining\", epochs=1)\n",
    "config.set_generation_parameters(\n",
    "    target_criterion=\"force_number_filtered\",\n",
    "    force_filters=\"ADMET+FGs\",\n",
    "    target_number=1_000,\n",
    ")\n",
    "\n",
    "config.verbose = True\n",
    "config.set_sampling_parameters(\n",
    "    n_clusters=10,\n",
    "    samples_per_cluster=2,\n",
    "    pca_fname=\"scaler_pca_combined_processed_freq1000_block133_120.pkl\",\n",
    ")\n",
    "config.set_scoring_parameters(protein_path=\"1ieb_processed.pdb\")\n",
    "config.set_active_learning_parameters(\n",
    "    selection_mode=\"threshold\", threshold=11, training_size=10_000\n",
    ")\n",
    "# model, trainer = Training.train_GPT(\n",
    "#     config=config,\n",
    "#     training_dataset=ds[0],\n",
    "# )\n",
    "# Generation.generate_smiles(config)\n",
    "# Generation.characterize_generated_molecules(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling.calculate_descriptors(config)\n",
    "# mols = Sampling.project_into_pca_space(config)\n",
    "# Sampling.cluster_and_sample(mols=mols, config=config, n_iter=1)\n",
    "# ALConstruction.construct_al_training_set(config=config, do_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The following AL training set construction parameters were set:\n",
      "    the training set will be constructed to have 10 molecules\n",
      "    of which 5 will be selected from the top scoring molecules defined by the following parameters:\n",
      "        molecules with score above 11 will be selected\n",
      "    the remaining 5 molecules will be selected from high-scoring clusters according to the following parameters:\n",
      "        the following probability mode will be used: linear\n",
      "    the training set will be saved to             5_ActiveLearning/training_sets/model0_al1_ch1.csv                               \n"
     ]
    }
   ],
   "source": [
    "config.set_active_learning_parameters(\n",
    "    selection_mode=\"threshold\",\n",
    "    probability_mode=\"linear\",\n",
    "    threshold=11,\n",
    "    training_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will load AL training set from 5_ActiveLearning/training_sets/model0_al0_ch1.csv\n"
     ]
    }
   ],
   "source": [
    "al_ds = Dataset.load_data(config=config, mode=\"Active Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The following training parameters were set:\n",
      "    number of epochs: 1\n",
      "    learning rate: 3e-05\n",
      "    learning warmup enabled? False\n",
      "    model weights will be loaded from:            1_Pretraining/model_weights/model0_al0_ch1.pt                                   \n",
      "    model weights will be saved to:               5_ActiveLearning/model_weights/model0_al1_ch1.pt                                \n",
      "    dataset descriptors will be loaded from:      1_Pretraining/datasets_descriptors/moses_train.yaml                             \n",
      "  . note: wandb_project_name and wandb_runname were not provided, you can ignore this message if you don't plan to log runs to wandb\n"
     ]
    }
   ],
   "source": [
    "config.set_training_parameters(mode=\"Active Learning\", epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morgunov/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/morgunov/batista/Summer/ChemSpaceAL/example_notebook.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/morgunov/batista/Summer/ChemSpaceAL/example_notebook.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model, trainer \u001b[39m=\u001b[39m Training\u001b[39m.\u001b[39;49mtrain_GPT(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morgunov/batista/Summer/ChemSpaceAL/example_notebook.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morgunov/batista/Summer/ChemSpaceAL/example_notebook.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     training_dataset\u001b[39m=\u001b[39;49mal_ds,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morgunov/batista/Summer/ChemSpaceAL/example_notebook.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n",
      "File \u001b[0;32m~/batista/Summer/ChemSpaceAL/ChemSpaceAL/Training.py:247\u001b[0m, in \u001b[0;36mtrain_GPT\u001b[0;34m(config, training_dataset, validation_dataset, load_checkpoint, log_wandb)\u001b[0m\n\u001b[1;32m    235\u001b[0m     wandb\u001b[39m.\u001b[39minit(\n\u001b[1;32m    236\u001b[0m         project\u001b[39m=\u001b[39mmconf\u001b[39m.\u001b[39mtrain_params[\u001b[39m\"\u001b[39m\u001b[39mwandb_project_name\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    237\u001b[0m         name\u001b[39m=\u001b[39mmconf\u001b[39m.\u001b[39mtrain_params[\u001b[39m\"\u001b[39m\u001b[39mwandb_runname\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m    240\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    241\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m     wandb\u001b[39m=\u001b[39mwandb \u001b[39mif\u001b[39;00m wandb\u001b[39m.\u001b[39mrun \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m )\n\u001b[0;32m--> 247\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    248\u001b[0m \u001b[39mif\u001b[39;00m log_wandb:\n\u001b[1;32m    249\u001b[0m     wandb\u001b[39m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/batista/Summer/ChemSpaceAL/ChemSpaceAL/Training.py:141\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m best_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config\u001b[39m.\u001b[39mtrain_params[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m--> 141\u001b[0m     train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_epoch(\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, epoch)\n\u001b[1;32m    142\u001b[0m     log_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mepoch_train_loss\u001b[39m\u001b[39m\"\u001b[39m: train_loss, \u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m: epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m}\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_dataset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/batista/Summer/ChemSpaceAL/ChemSpaceAL/Training.py:86\u001b[0m, in \u001b[0;36mTrainer.run_epoch\u001b[0;34m(self, split, epoch)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m is_train:\n\u001b[1;32m     84\u001b[0m     \u001b[39m# Gradient accumulation and optimization\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 86\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     87\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39munscale_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer)\n\u001b[1;32m     88\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), \u001b[39m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, trainer = Training.train_GPT(\n",
    "    config=config,\n",
    "    training_dataset=al_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/morgunov/batista/Summer/ChemSpaceAL/publication_runs/5_ActiveLearning/training_sets/model0_al0_ch1.csv'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.cycle_temp_params[\"path_to_al_training_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/morgunov/batista/Summer/ChemSpaceAL/publication_runs/5_ActiveLearning/training_sets/model0_al1_ch1.csv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"/Users/morgunov/batista/Summer/ChemSpaceAL/publication_runs/5_ActiveLearning/training_sets/model0_al0_ch1.csv\".replace(\n",
    "    f\"al0\", f\"al1\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
