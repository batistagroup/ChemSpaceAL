{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/batistagroup/ChemSpaceAL/packaging/media/logo.png)"
      ],
      "metadata": {
        "id": "cq2uUJd4vasb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Colab notebook allows you to easily implement [ChemSpaceAL v2.0.0](https://github.com/batistagroup/ChemSpaceAL/). For more details, check out the associated [preprint on arXiv](https://arxiv.org/abs/2309.05853). Please feel free to start any discussion or raise any issues on our [GitHub](https://github.com/batistagroup/ChemSpaceAL/)."
      ],
      "metadata": {
        "id": "JIXuCJuWvxPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/batistagroup/ChemSpaceAL/packaging/media/toc_figure.jpg)"
      ],
      "metadata": {
        "id": "vM4RkGcWvPnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#@title Set Up Notebook\n",
        "#@markdown Press the *Play* button to install ChemSpaceAL and its dependencies\n",
        "\n",
        "!rm -r ChemSpaceAL\n",
        "!git clone --single-branch --branch packaging https://github.com/batistagroup/ChemSpaceAL\n",
        "!git clone https://github.com/Liuhong99/Sophia.git\n",
        "!pip install ChemSpaceAL/.\n",
        "\n",
        "import ChemSpaceAL\n",
        "from ChemSpaceAL import InitializeWorkspace\n",
        "from ChemSpaceAL import Configuration\n",
        "from ChemSpaceAL import Dataset\n",
        "from ChemSpaceAL import Model\n",
        "from ChemSpaceAL import Training\n",
        "from ChemSpaceAL import Generation\n",
        "from ChemSpaceAL import Sampling\n",
        "from ChemSpaceAL import ALConstruction\n",
        "base_path = None\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZaOS8lmivRyk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the Workspace"
      ],
      "metadata": {
        "id": "TFYfz0Fjw_Qr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As implemented in the paper, one iteration of ChemSpaceAL takes ~22 hours, of which 20 are spent on docking. To ensure no data is lost due to accidental termination of Colab sessions, we strongly recommend connecting Google Drive and creating a dedicated folder for storing results of runs.\n",
        "\n",
        "If you do not execute the cell below, all files will be saved within local sessions and will be lost if the session terminates!\n",
        "\n",
        "Please note that this package has been optimized for running multiple AL iterations. Practically, this means that the code may seem daunting to you at first, but once you get familiar with it, you can analyze the results of scoring from some AL iteration and launch the docking for the next iteration within 30 min! Feel free [to reach out to us](https://github.com/batistagroup/ChemSpaceAL/) if you need help with getting started!"
      ],
      "metadata": {
        "id": "LlBHtsxExBg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Specify (base) path for storing results\n",
        "# @markdown make sure your path ends with a \"/\"\n",
        "base_path = \"/content/drive/MyDrive/ChemSpaceAL-runs/\" # @param {type:\"string\"}\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "ZrO0vcmN5tJl",
        "outputId": "a7e06364-c9a1-4359-fa70-b5970c22f245"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create subfolders for storing results\n",
        "By default, the following folder structure will be created\n",
        "\n",
        "```\n",
        "ChemSpaceAL-runs\n",
        "├── 1_Pretraining\n",
        "│   ├── dataset_folder: datasets\n",
        "│   ├── desc_folder: datasets_descriptors\n",
        "│   └── weight_folder: model_weights\n",
        "├── 2_Generation\n",
        "├── 3_Sampling\n",
        "│   ├── desc_folder: generations_descriptors\n",
        "│   ├── pca_folder: pca_weights\n",
        "│   ├── kmeans_folder: kmeans_objects\n",
        "│   └── clustering_folder: clusterings\n",
        "├── 4_Scoring\n",
        "│   ├── target_folder: binding_targets\n",
        "│   ├── candidate_folder: sampled_mols\n",
        "│   ├── pose_folder: binding_poses\n",
        "│   └── score_folder: scored_dataframes\n",
        "└── 5_ActiveLearning\n",
        "    ├── train_folder: training_sets\n",
        "    ├── desc_folder: trainingset_descriptors\n",
        "    └── weight_folder: model_weights\n",
        "```\n",
        "\n",
        "This structure is specified by `InitializeWorkspace.FOLDER_STRUCTURE`. The values to keys `*_folder` are the folder names. You're more than welcome them to rename if you want and pass a new folder_structure dict to `InitializeWorkspace.create_folders` as an optional parameter `folder_structure=`."
      ],
      "metadata": {
        "id": "MKLEfaEG8Mg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title create subfolders\n",
        "#@markdown By default, the following folder structure will be created\n",
        "if base_path is None:\n",
        "  base_path = os.getcwd() + \"/runs/\"\n",
        "InitializeWorkspace.create_folders(base_path=base_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "bm3jRgP-w0SX",
        "outputId": "0dfaa0c2-0528-4202-f6eb-b09c85425993"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    will create folders at base_path='/content/drive/MyDrive/ChemSpaceAL-runs/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download (if you want) dataset/weights\n",
        "#@markdown note these files will be placed into appropriate folders created above\n",
        "downloadDataset = False # @param {type:\"boolean\"}\n",
        "downloadModelWeights = True # @param {type:\"boolean\"}\n",
        "downloadPCAweights = True # @param {type:\"boolean\"}\n",
        "script = '''#!/bin/bash\n",
        "'''\n",
        "remote_source = \"https://files.ischemist.com/ChemSpaceAL/publication_runs/\"\n",
        "if downloadDataset:\n",
        "  f1 = \"1_Pretraining/datasets/combined_train.csv.gz\"\n",
        "  f2 = \"1_Pretraining/datasets/combined_valid.csv.gz\"\n",
        "  script += f\"curl -o {base_path}{f1} {remote_source}{f1}\\n\"\n",
        "  script += f\"curl -o {base_path}{f2} {remote_source}{f2}\\n\"\n",
        "if downloadModelWeights:\n",
        "  f1 = \"1_Pretraining/datasets_descriptors/combined_train.yaml\"\n",
        "  f2 = \"1_Pretraining/model_weights/model7_al0_ch1.pt\"\n",
        "  script += f\"curl -o {base_path}{f1} {remote_source}{f1}\\n\"\n",
        "  script += f\"curl -o {base_path}{f2} {remote_source}{f2}\\n\"\n",
        "if downloadPCAweights:\n",
        "  f1 = \"3_Sampling/pca_weights/scaler_pca_combined_n120.pkl\"\n",
        "  script += f\"curl -o {base_path}{f1} {remote_source}{f1}\\n\"\n",
        "with open(\"fetch.bash\", \"w\") as f:\n",
        "  f.write(script)\n",
        "!bash fetch.bash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "8vVlLnpXxzAM",
        "outputId": "e4c615a1-52b0-4873-b9e5-5cd6f457583f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1357  100  1357    0     0   6238      0 --:--:-- --:--:-- --:--:--  6253\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 24.9M  100 24.9M    0     0  45.5M      0 --:--:-- --:--:-- --:--:-- 45.5M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  208k  100  208k    0     0  1056k      0 --:--:-- --:--:-- --:--:-- 1057k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Active Learning Iteration"
      ],
      "metadata": {
        "id": "xm1l1QtQ9C_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we have to initialize a global config. A brief explanation of some of the parameters.\n",
        "\n",
        "`cycle_prefix`, `al_iteration`, and `cycle_suffix` are used to compose filenames for all results and intermediary output. For example, with default parameters, all filenames will start as `model0_al0_ch1`. We recommend changing `cycle_prefix` for different pretrained generative models, changing `cycle_suffix` for different settings of AL, and you **must** not forget to increment `al_iteration` as you progress through AL cycles.\n",
        "\n",
        "`training_fname` and `validation_fname`"
      ],
      "metadata": {
        "id": "0aRfJ-qr9gV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting parameters"
      ],
      "metadata": {
        "id": "-f0s_Wxe372P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = Configuration.Config(\n",
        "    base_path=base_path,\n",
        "    cycle_prefix=\"model0\",\n",
        "    cycle_suffix=\"ch1\",\n",
        "    al_iteration=1, # use 0 for pretraining\n",
        "    training_fname=\"combined_train.csv.gz\",\n",
        "    validation_fname=\"combined_valid.csv.gz\",\n",
        "    slice_data=1,\n",
        "    verbose=True, # will print every important decision that's going to be made\n",
        ")\n",
        "# The following fills two lists (or you could provide them manually as optional parameters)\n",
        "# `previously_scored_mols` - a list of strings of paths to previously scored molecules\n",
        "# `previous_al_train_sets` - a list of strings of paths to AL training sets from previous iterations\n",
        "# these lists are needed to\n",
        "# - assess how many of generated molecules are repeated from AL training sets\n",
        "# - make sure already scored molecules are not sampled again for docking\n",
        "# notably, if you do not mess with the default naming system, these lists will be filled for you automatically\n",
        "# once you change al_iteration to a non-zero value!\n",
        "config.set_previous_arrays()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTtBaQrK9Uyt",
        "outputId": "b58d7fa7-25d9-465c-fef0-ed7f8a24e87d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- The following previously scored molecules were set:\n",
            "     4_Scoring/scored_dataframes/model0_al0_ch1.csv\n",
            "--- The following previously constructed Active Learning sets were set:\n",
            "     5_ActiveLearning/training_sets/model0_al0_ch1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretraining"
      ],
      "metadata": {
        "id": "90FvuJD03_ZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mode can be set to \"Pretraining\" or \"Active Learning\".\n",
        "# In \"Pretraining\", an output is a list of two dataset objects corrresponding to (train, valid) partitions\n",
        "# In \"Active Learning\", an output is a single dataset object corresponding to an AL training set\n",
        "datasets = Dataset.load_data(config=config, mode=\"Pretraining\")"
      ],
      "metadata": {
        "id": "gizQ7Vow-1Nd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also overwrite `learning_rate`, `lr_warmup` (a boolean of whether to do lr warmup),\n",
        "# For a full list of available parameters run help(config.set_training_parameters)\n",
        "config.set_training_parameters(mode=\"Pretraining\", epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyXNSHq2zxcj",
        "outputId": "f3a17b38-e30e-46af-ef4d-495f00387d46"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- The following training parameters were set:\n",
            "    number of epochs: 1\n",
            "    learning rate: 0.0003\n",
            "    learning warmup enabled? True\n",
            "    model weights will be saved to:               1_Pretraining/model_weights/model0_al1_ch1.pt                                   \n",
            "    dataset descriptors will be loaded from:      1_Pretraining/datasets_descriptors/combined_train.yaml                          \n",
            "  . note: wandb_project_name and wandb_runname were not provided, you can ignore this message if you don't plan to log runs to wandb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model objects and trainer objects are returned in case you want to do something with them\n",
        "model, trainer = Training.train_GPT(\n",
        "    config=config,\n",
        "    training_dataset=datasets[0],\n",
        "    validation_dataset=datasets[1]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytrX5o7Z0qJ3",
        "outputId": "e8b1e0be-6729-46e1-8956-ea8e07ff5649"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "epoch 1 iter 0: train loss 3.00481. lr 3.000000e-05: 100%|██████████| 1/1 [00:03<00:00,  3.38s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "6vnwS1RZ4BOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config.set_generation_parameters(\n",
        "    target_criterion=\"force_number_filtered\", # or you could choose `force_number_unique` or `force_number_completions`\n",
        "    force_filters=\"ADMET+FGs\", # could choose `ADMET` for no restriction on functional groups or simply remove this parameter\n",
        "    target_number=1,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSB3TfwA10t5",
        "outputId": "3aa6ee29-61fc-4e07-8937-96628a0a3b95"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- The following generation parameters were set:\n",
            "    target number: 1 unique canonical smiles that pass filters\n",
            "    batch size: 64 & temperature: 1.0\n",
            "    the following filters will be applied: ADMET+FGs\n",
            "    model weights will be loaded from:            1_Pretraining/model_weights/model0_al1_ch1.pt                                   \n",
            "    dataset descriptors will be loaded from:      1_Pretraining/datasets_descriptors/combined_train.yaml                          \n",
            "    generated completions will be saved to:       2_Generation/model0_al1_ch1_completions.csv                                     \n",
            "    unique canonic smiles will be saved to:       2_Generation/model0_al1_ch1_unique_smiles.csv                                   \n",
            "    generation metrics will be saved to:          2_Generation/model0_al1_ch1_metrics.txt                                         \n",
            "    filtered molecules will be saved to:          2_Generation/model0_al1_ch1_filtered_smiles.csv                                 \n",
            "    The following ADMET filters will be enforced:\n",
            "    |    MW in range [100, 600]\n",
            "    |    nHA in range [0, 12]\n",
            "    |    nHD in range [0, 7]\n",
            "    |    nRot in range [0, 11]\n",
            "    |    nRing in range [0, 6]\n",
            "    |    nHet in range [1, 15]\n",
            "    |    fChar in range [-4, 4]\n",
            "    |    TPSA in range [0, 140]\n",
            "    |    logP in range [-0.4, 6.5]\n",
            "    The following functional groups will be restricted:\n",
            "    |    fr_azide, fr_isocyan, fr_isothiocyan, fr_nitro, fr_nitro_arom,\n",
            "    |    fr_nitro_arom_nonortho, fr_nitroso, fr_phos_acid, fr_phos_ester,\n",
            "    |    fr_sulfonamd, fr_sulfone, fr_term_acetylene, fr_thiocyan,\n",
            "    |    fr_prisulfonamd, fr_C_S, fr_azo, fr_diazo, fr_epoxide, fr_ester,\n",
            "    |    fr_COO2, fr_Imine, fr_N_O, fr_SH, fr_aldehyde, fr_dihydropyridine,\n",
            "    |    fr_hdrzine, fr_hdrzone, fr_ketone, fr_thiophene, fr_phenol\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Generation.generate_smiles(config) # this runs generation of SMILES\n",
        "Generation.characterize_generated_molecules(config) # this runs an analysis of # unique, valid, and novel molecules"
      ],
      "metadata": {
        "id": "-g1fCwHI3pUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling"
      ],
      "metadata": {
        "id": "0QKvwE3T5SnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config.set_sampling_parameters(\n",
        "    n_clusters=10,\n",
        "    samples_per_cluster=2,\n",
        "    pca_fname=\"scaler_pca_combined_n120.pkl\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDp1SaQB5TBX",
        "outputId": "47f67014-093c-42b4-bd0e-0ce6342016cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- The following sampling parameters were set:\n",
            "    number of clusters: 10\n",
            "    samples per cluster: 2\n",
            "    descriptors mode: mix\n",
            "    descriptors will be saved to:                      3_Sampling/generations_descriptors/model0_al1_ch1.pkl                           \n",
            "    PCA will be loaded from:                           3_Sampling/pca_weights/scaler_pca_combined_n120.pkl                             \n",
            "    KMeans Objects will be saved to:                   3_Sampling/kmeans_objects/model0_al1_ch1_k10.pkl                                \n",
            "    cluster to molecules mapping will be saved to:     3_Sampling/clusterings/model0_al1_ch1_cluster_to_mols.pkl                       \n",
            "    sampled molecules will be saved to:                4_Scoring/sampled_mols/model0_al1_ch1_sampled20.csv                             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Sampling.calculate_descriptors(config)\n",
        "mols = Sampling.project_into_pca_space(config)\n",
        "Sampling.cluster_and_sample(mols=mols, config=config, n_iter=1)"
      ],
      "metadata": {
        "id": "hdqp-6Li6PfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scoring"
      ],
      "metadata": {
        "id": "yKCe3_uK5TXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#@title Install Docking Software (DiffDock)\n",
        "#@markdown diffdock is pretty heavy and has a lot of dependencies, so we only install it when we need it (and we don't during pretraining, for example)\n",
        "\n",
        "import torch\n",
        "\n",
        "print(torch.__version__)\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git  --quiet\n",
        "\n",
        "try:\n",
        "    import biopandas\n",
        "except:\n",
        "    !pip install pyg==0.7.1 --quiet\n",
        "    !pip install pyyaml==6.0 --quiet\n",
        "    !pip install scipy==1.7.3 --quiet\n",
        "    !pip install networkx==2.6.3 --quiet\n",
        "    !pip install biopython==1.79 --quiet\n",
        "    !pip install rdkit-pypi==2022.03.5 --quiet\n",
        "    !pip install e3nn==0.5.0 --quiet\n",
        "    !pip install spyrmsd==0.5.2 --quiet\n",
        "    !pip install pandas==1.5.3 --quiet\n",
        "    !pip install biopandas==0.4.1 --quiet\n",
        "\n",
        "if not os.path.exists(\"/content/DiffDock\"):\n",
        "    os.chdir('/content')\n",
        "    !git clone https://github.com/gcorso/DiffDock.git\n",
        "    os.chdir('/content/DiffDock')\n",
        "    !git checkout a6c5275\n",
        "    os.chdir('/content')\n",
        "\n",
        "if not os.path.exists(\"/content/DiffDock/esm\"):\n",
        "    os.chdir('/content/DiffDock')\n",
        "    !git clone https://github.com/facebookresearch/esm\n",
        "    os.chdir('/content/DiffDock/esm')\n",
        "    !git checkout ca8a710\n",
        "    !sudo pip install -e .\n",
        "    os.chdir('/content/DiffDock')\n",
        "    os.chdir('/content')\n",
        "\n",
        "from rdkit import Chem\n",
        "import shutil\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bjes5o_e6v_2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ChemSpaceAL.Docking import get_top_poses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT-N3o145UdV",
        "outputId": "23b1379e-9f8d-40fe-8084-6eada4d38480"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.set_scoring_parameters(\n",
        "    protein_path=\"HNH_processed.pdb\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj43Lidy9MWx",
        "outputId": "8503eafc-eab0-47a9-a0e7-2121b59d9f50"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- The following scoring parameters were set:\n",
            "    Reminder that docking poses will be written to 4_Scoring/binding_poses/                                                        \n",
            "    protein will be loaded from                   4_Scoring/binding_targets/HNH_processed.pdb                                     \n",
            "    poses will be saved to                        4_Scoring/binding_poses/model0_al1_ch1/                                         \n",
            "    and scored molecules will be saved to         4_Scoring/scored_dataframes/model0_al1_ch1.csv                                  \n",
            "    The following prolif interaction weights will be used:\n",
            "    |    Hydrophobic: 2.5, HBDonor: 3.5, HBAcceptor: 3.5, Anionic: 7.5, Cationic: 7.5,\n",
            "    |    CationPi: 2.5, PiCation: 2.5, VdWContact: 1.0, XBAcceptor: 3.0,\n",
            "    |    XBDonor: 3.0, FaceToFace: 3.0, EdgeToFace: 1.0, MetalDonor: 3.0,\n",
            "    |    MetalAcceptor: 3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Docking"
      ],
      "metadata": {
        "id": "GChMeujNDErG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_top_poses(\n",
        "    ligands_csv=config.cycle_temp_params[\"path_to_sampled\"],\n",
        "    protein_pdb_path=config.cycle_temp_params[\"path_to_protein\"],\n",
        "    save_pose_path=config.cycle_temp_params[\"path_to_poses\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AAW56ri8-LZ",
        "outputId": "817f3521-3d17-4329-de1d-a3929bdcd15f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 1515.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting attractive interaction scores"
      ],
      "metadata": {
        "id": "Pj6fJycjDIzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ChemSpaceAL import Scoring\n",
        "ligand_scores = Scoring.score_ligands(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_CatI-oDD-T",
        "outputId": "4b386f98-898b-41e8-e095-7aaefdbcdd02"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:07<00:00,  2.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Scoring.parse_and_prepare_diffdock_data(\n",
        "    ligand_scores=ligand_scores,\n",
        "    config=config\n",
        ")"
      ],
      "metadata": {
        "id": "r8DBlUedBC5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Active Learning"
      ],
      "metadata": {
        "id": "7bTKgdFB5U1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config.set_active_learning_parameters(\n",
        "    selection_mode=\"threshold\", probability_mode=\"linear\", threshold=11, training_size=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNnNENATEYeX",
        "outputId": "d395ac8e-a970-469e-eb09-90dd71ab3331"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- The following AL training set construction parameters were set:\n",
            "    the training set will be constructed to have 10 molecules\n",
            "    of which 5 will be selected from the top scoring molecules defined by the following parameters:\n",
            "        molecules with score above 11 will be selected\n",
            "    the remaining 5 molecules will be selected from high-scoring clusters according to the following parameters:\n",
            "        the following probability mode will be used: linear\n",
            "    the training set will be saved to             5_ActiveLearning/training_sets/model0_al1_ch1.csv                               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ALConstruction.construct_al_training_set(config=config, do_sampling=True)"
      ],
      "metadata": {
        "id": "2j6MiofeEOzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "al_ds = Dataset.load_data(config=config, mode=\"Active Learning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbkAwpOzGK61",
        "outputId": "7e5f0804-8be2-42d5-9952-19ca3a33ab4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will load AL training set from 5_ActiveLearning/training_sets/model0_al0_ch1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.set_training_parameters(mode=\"Active Learning\", epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iphPbepYHYIr",
        "outputId": "fd1ff31d-3f40-4245-8101-15c11a86cae5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- The following training parameters were set:\n",
            "    number of epochs: 1\n",
            "    learning rate: 3e-05\n",
            "    learning warmup enabled? False\n",
            "    model weights will be loaded from:            1_Pretraining/model_weights/model0_al0_ch1.pt                                   \n",
            "    model weights will be saved to:               5_ActiveLearning/model_weights/model0_al1_ch1.pt                                \n",
            "    dataset descriptors will be loaded from:      1_Pretraining/datasets_descriptors/combined_train.yaml                          \n",
            "  . note: wandb_project_name and wandb_runname were not provided, you can ignore this message if you don't plan to log runs to wandb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, trainer = Training.train_GPT(\n",
        "    config=config,\n",
        "    training_dataset=al_ds,\n",
        ")"
      ],
      "metadata": {
        "id": "KePj2bHZ37wR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}